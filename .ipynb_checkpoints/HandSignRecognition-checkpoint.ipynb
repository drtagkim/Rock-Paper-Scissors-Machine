{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hand Sing Recognition\n",
    "## 참고\n",
    "* https://google.github.io/mediapipe/\n",
    "* https://github.com/kairess/Rock-Paper-Scissors-Machine\n",
    "\n",
    "## 작성자\n",
    "* 김태경, PhD\n",
    "* 광운대학교 경영학부, MIS 부교수\n",
    "* 2021/05/25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_hands = mp.solutions.hands #hand solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_model('data/gesture_train.csv','hand_recognition_model.xml')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HandSign:\n",
    "    gesture = {\n",
    "        0:'fist', 1:'one', 2:'two', 3:'three', 4:'four', 5:'five',\n",
    "        6:'six', 7:'rock', 8:'spiderman', 9:'yeah', 10:'ok',\n",
    "    }\n",
    "    numbers = {\n",
    "        0:'zero', 1:'one', 2:'two', 3:'three', 4:'four', 5:'five',\n",
    "        6:'ready', 9:'two', 10:'three',\n",
    "    }\n",
    "    rps_gesture = {0:'rock', 5:'paper', 9:'scissors'}\n",
    "    def __init__(self,model):\n",
    "        self.model_file=model\n",
    "        self.model=self.load_model(model)\n",
    "    def watch(self,sign=None,detection_conf=0.8,tracking_conf=0.6):\n",
    "        sign=self.rps_gesture\n",
    "        knn=self.model\n",
    "        hands=self.hand_recognition(2,detection_conf,tracking_conf)\n",
    "        cap = cv2.VideoCapture(0)\n",
    "        window_name=\"Test Hand Recognition\"\n",
    "        while cap.isOpened():\n",
    "            ret, img = cap.read() #video read\n",
    "            if not ret:\n",
    "                continue #not ready yet\n",
    "            # ----------------------------------------------\n",
    "            cv2.namedWindow(window_name, cv2.WND_PROP_FULLSCREEN)\n",
    "            cv2.setWindowProperty(window_name,cv2.WND_PROP_FULLSCREEN,\n",
    "               cv2.WINDOW_FULLSCREEN)\n",
    "            img=self.preprocess_img(img)\n",
    "            result=hands.process(img)\n",
    "            img=self.postprocess_img(img)\n",
    "            #\n",
    "            width=img.shape[1]\n",
    "            height=img.shape[0]\n",
    "            self.print_head(img,\"Type q to exit\",x=width*0.02,y=height*0.1)\n",
    "            #self.\n",
    "            #\n",
    "            if result.multi_hand_landmarks is not None:\n",
    "                for res in result.multi_hand_landmarks:\n",
    "                    mp_drawing.draw_landmarks(img, res, mp_hands.HAND_CONNECTIONS)\n",
    "                    idx=self.get_label(knn,res)\n",
    "                    # Draw gesture result\n",
    "                    if idx in sign.keys():\n",
    "                        cv2.putText(img,\n",
    "                                    text=sign[idx].upper(),\n",
    "                                    org=(int(res.landmark[0].x * img.shape[1]), #width\n",
    "                                         int(res.landmark[0].y * img.shape[0] + 20)), #height\n",
    "                                    fontFace=cv2.FONT_HERSHEY_SIMPLEX, \n",
    "                                    fontScale=1, \n",
    "                                    color=(255, 255, 255), \n",
    "                                    thickness=2)\n",
    "                    # Other gestures\n",
    "                    cv2.putText(img, \n",
    "                                text=self.gesture[idx].upper(), \n",
    "                                org=(int(res.landmark[0].x * img.shape[1]), \n",
    "                                     int(res.landmark[0].y * img.shape[0]*1.1 + 20)), \n",
    "                                fontFace=cv2.FONT_HERSHEY_SIMPLEX, \n",
    "                                fontScale=1, color=(0, 255, 0), \n",
    "                                thickness=2)\n",
    "            cv2.imshow(window_name, img)\n",
    "            if cv2.waitKey(1) == ord('q'):\n",
    "                cv2.destroyAllWindows() #close the image window\n",
    "                cap.release()\n",
    "                break    \n",
    "    def hand_recognition(self,max_num_hands=1,detection_conf=0.5,tracking_conf=0.5):\n",
    "        hands = mp_hands.Hands(\n",
    "            max_num_hands=max_num_hands,\n",
    "            min_detection_confidence=detection_conf,\n",
    "            min_tracking_confidence=tracking_conf)\n",
    "        return hands\n",
    "    def train_model(self,data_file,save='hand_recognition_model.xml'):\n",
    "        file = np.genfromtxt(data_file, delimiter=',')\n",
    "        angle = file[:,:-1].astype(np.float32)\n",
    "        label = file[:, -1].astype(np.float32)\n",
    "        knn = cv2.ml.KNearest_create()\n",
    "        knn.train(angle, cv2.ml.ROW_SAMPLE, label)\n",
    "        knn.save(save)\n",
    "    def load_model(self,file_name):\n",
    "        return cv2.ml.KNearest_load(file_name)\n",
    "    def print_head(self,img,text,x=50,y=50):\n",
    "        cv2.putText(img, \n",
    "                    text, org=(int(x),int(y)),\n",
    "                    fontFace=cv2.FONT_HERSHEY_SIMPLEX, \n",
    "                    fontScale=0.5, \n",
    "                    color=(255, 255, 255),\n",
    "                    thickness=1)\n",
    "    def finger_vectors(self,landmarks):\n",
    "        joint=np.zeros((21,3,))\n",
    "        for j, lm in enumerate(landmarks): #landmark data\n",
    "            joint[j] = [lm.x, lm.y, lm.z]\n",
    "        N=len(landmarks)\n",
    "        v1 = joint[[0,1,2,3,0,5,6,7,0,9,10,11,0,13,14,15,0,17,18,19],:] # Parent joint\n",
    "        v2 = joint[[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20],:] # Child joint\n",
    "        v = v2 - v1 # [20,3] #vector (x,y,z)\n",
    "        v = v / np.linalg.norm(v, axis=1)[:, np.newaxis] #normalize, 길이로 나눈다. Norm\n",
    "        return v\n",
    "    def cal_angles(self,finger_vecs):\n",
    "        angle = np.arccos(np.einsum('ij,ij->i', #row sum\n",
    "            finger_vecs[[0,1,2,4,5,6,8,9,10,12,13,14,16,17,18],:], \n",
    "            finger_vecs[[1,2,3,5,6,7,9,10,11,13,14,15,17,18,19],:])) # [15,]\n",
    "        angle = np.degrees(angle) # Convert radian to degree\n",
    "        data = np.array([angle], dtype=np.float32)\n",
    "        return data\n",
    "\n",
    "    def get_label(self,knn_model,res):\n",
    "        v=self.finger_vectors(res.landmark)\n",
    "        data=self.cal_angles(v)\n",
    "        _, results, _, _ = knn_model.findNearest(data, 3) #res,result,neighbors,distances\n",
    "        try:\n",
    "            idx = int(results[0][0])\n",
    "        except:\n",
    "            idx=None\n",
    "        return idx\n",
    "    def preprocess_img(self,img):\n",
    "        img = cv2.flip(img, 1)\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB) #색공간 변경\n",
    "        return img\n",
    "    def postprocess_img(self,img):\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)\n",
    "        return img    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "hand_sign=HandSign('hand_recognition_model.xml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'gesture' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-27-d3261d31055c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mhand_sign\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mHandSign\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumbers\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-24-1cf5e442b74c>\u001b[0m in \u001b[0;36mwatch\u001b[1;34m(self, sign, detection_conf, tracking_conf)\u001b[0m\n\u001b[0;32m     51\u001b[0m                     \u001b[1;31m# Other gestures\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     52\u001b[0m                     cv2.putText(img, \n\u001b[1;32m---> 53\u001b[1;33m                                 \u001b[0mtext\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mgesture\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     54\u001b[0m                                 org=(int(res.landmark[0].x * img.shape[1]), \n\u001b[0;32m     55\u001b[0m                                      int(res.landmark[0].y * img.shape[0]*1.1 + 20)), \n",
      "\u001b[1;31mNameError\u001b[0m: name 'gesture' is not defined"
     ]
    }
   ],
   "source": [
    "hand_sign.watch(HandSign.numbers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
